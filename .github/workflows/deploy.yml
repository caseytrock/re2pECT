name: Infrastructure and Deployment Pipeline

on:
  push:
    branches: [main]

env:
  AWS_REGION: 'us-west-2'
  FLASK_PORT: '5000'
  DOCKER_IMAGE: ghcr.io/${{ github.repository_owner }}/re2ect-app:latest

jobs:
  provision-infrastructure:
    runs-on: ubuntu-latest
    outputs:
      ec2_ip: ${{ steps.get-outputs.outputs.public_ip }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init
        run: terraform init
        working-directory: ./terraform

      - name: Terraform Apply (Smart Update)
        id: tf-apply
        run: |
          if terraform apply -refresh-only -auto-approve \
              -var="ghcr_username=${{ github.repository_owner }}" \
              -var="ghcr_token=${{ secrets.GITHUB_TOKEN }}" \
              -input=false; then
            echo "Existing infrastructure refreshed"
          else
            echo "Performing full apply..."
            terraform apply -auto-approve \
              -var="ghcr_username=${{ github.repository_owner }}" \
              -var="ghcr_token=${{ secrets.GITHUB_TOKEN }}" \
              -input=false
          fi
        working-directory: ./terraform

      - name: Get Public IP
        id: get-outputs
        run: |
          IP=$(terraform -chdir=./terraform output -raw public_ip)
          echo "public_ip=${IP}" >> $GITHUB_OUTPUT
          echo "Using EC2 instance: ${IP}"

      - name: Prepare SSH Key
        if: ${{ !failure() }}
        run: |
          mkdir -p ./ssh_artifacts
          terraform -chdir=./terraform output -raw ssh_private_key > ./ssh_artifacts/deploy_key
          chmod 600 ./ssh_artifacts/deploy_key

      - name: Upload SSH Key
        if: ${{ !failure() }}
        uses: actions/upload-artifact@v4
        with:
          name: ssh-key
          path: ./ssh_artifacts/deploy_key
          retention-days: 1

  deploy-application:
    needs: provision-infrastructure
    runs-on: ubuntu-latest
    steps:
      - name: Verify EC2 Exists
        run: |
          if [ -z "${{ needs.provision-infrastructure.outputs.ec2_ip }}" ]; then
            echo "::error::No EC2 instance available. Infrastructure may need initialization - push again."
            exit 1
          fi
          echo "Using EC2 instance: ${{ needs.provision-infrastructure.outputs.ec2_ip }}"

      - name: Download SSH Key
        uses: actions/download-artifact@v4
        with:
          name: ssh-key
          path: ~/.ssh/

      - name: Configure SSH
        run: |
          chmod 700 ~/.ssh
          chmod 600 ~/.ssh/deploy_key
          echo "StrictHostKeyChecking no" >> ~/.ssh/config

      - name: Verify k3s Readiness
        run: |
          ssh -i ~/.ssh/deploy_key \
            ec2-user@${{ needs.provision-infrastructure.outputs.ec2_ip }} \
            "until kubectl get nodes &>/dev/null; do sleep 5; echo 'Waiting for k3s...'; done"

      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and Push Docker Image
        uses: docker/build-push-action@v5
        with:
          context: ./app
          push: true
          tags: ${{ env.DOCKER_IMAGE }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Configure GHCR Auth for Containerd
        run: |
          IP=${{ needs.provision-infrastructure.outputs.ec2_ip }}
          ssh -i ~/.ssh/deploy_key ec2-user@$IP << 'EOF'
          sudo mkdir -p /etc/rancher/k3s/
          cat << 'EOL' | sudo tee /etc/rancher/k3s/registries.yaml
          mirrors:
            ghcr.io:
              endpoint:
                - "https://ghcr.io"
          configs:
            "ghcr.io":
              auth:
                username: "${{ github.repository_owner }}"
                password: "${{ secrets.GITHUB_TOKEN }}"
          EOL
          sudo systemctl restart k3s
          EOF

      - name: Install and Configure Traefik
        run: |
          IP=${{ needs.provision-infrastructure.outputs.ec2_ip }}
          ssh -i ~/.ssh/deploy_key ec2-user@$IP << 'EOF'
          # Ensure Traefik is properly installed
          sudo kubectl apply -f - <<EOL
          apiVersion: helm.cattle.io/v1
          kind: HelmChart
          metadata:
            name: traefik
            namespace: kube-system
          spec:
            chart: traefik
            repo: https://helm.traefik.io/traefik
            targetNamespace: kube-system
            valuesContent: |-
              deployment:
                kind: DaemonSet
                hostNetwork: true
              ports:
                web:
                  port: 80
                  hostPort: 80
                  expose: true
                  protocol: TCP
              service:
                type: ClusterIP
              providers:
                kubernetesIngress:
                  publishedService:
                    enabled: true
          EOL
          
          # Wait for Traefik to be ready
          until kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik | grep Running; do
            sleep 5
            echo "Waiting for Traefik..."
          done
          
          # Verify port binding
          until sudo netstat -tulnp | grep -E ':80\s.*traefik'; do
            sleep 3
            echo "Waiting for Traefik to bind to port 80..."
          done
          EOF

      - name: Verify Network Configuration
        run: |
          IP=${{ needs.provision-infrastructure.outputs.ec2_ip }}
          ssh -i ~/.ssh/deploy_key ec2-user@$IP << 'EOF'
          echo "=== Network Status ==="
          sudo netstat -tulnp | grep -E '80|443'
          echo "=== Traefik Pod ==="
          kubectl get pods -n kube-system -l app.kubernetes.io/name=traefik -o wide
          echo "=== Traefik Logs ==="
          kubectl logs -n kube-system -l app.kubernetes.io/name=traefik --tail=50
          EOF

      - name: Deploy Application
        run: |
          cat <<EOF > deploy.yaml
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: re2ect-app
          spec:
            replicas: 2
            selector:
              matchLabels:
                app: re2ect-app
            template:
              metadata:
                labels:
                  app: re2ect-app
              spec:
                containers:
                - name: app
                  image: ${{ env.DOCKER_IMAGE }}
                  ports:
                  - containerPort: ${{ env.FLASK_PORT }}
                  readinessProbe:
                    httpGet:
                      path: /
                      port: ${{ env.FLASK_PORT }}
          ---
          apiVersion: v1
          kind: Service
          metadata:
            name: re2ect-app-service
          spec:
            ports:
            - port: 80
              targetPort: ${{ env.FLASK_PORT }}
            selector:
              app: re2ect-app
          ---
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: re2ect-app-ingress
            annotations:
              traefik.ingress.kubernetes.io/router.entrypoints: web
          spec:
            rules:
            - http:
                paths:
                - path: /
                  pathType: Prefix
                  backend:
                    service:
                      name: re2ect-app-service
                      port:
                        number: 80
          EOF

          scp -i ~/.ssh/deploy_key deploy.yaml ec2-user@${{ needs.provision-infrastructure.outputs.ec2_ip }}:~/ 
          ssh -i ~/.ssh/deploy_key ec2-user@${{ needs.provision-infrastructure.outputs.ec2_ip }} \
            "kubectl apply -f deploy.yaml"

      - name: Verify Deployment
        run: |
          IP=${{ needs.provision-infrastructure.outputs.ec2_ip }}
          ssh -i ~/.ssh/deploy_key ec2-user@$IP << 'EOF'
          kubectl rollout status deployment/re2ect-app --timeout=180s
          echo "=== Cluster Status ==="
          kubectl get pods,svc,ingress -A
          EOF

      - name: Test Application
        run: |
          IP=${{ needs.provision-infrastructure.outputs.ec2_ip }}
          echo "Testing application at http://$IP"
          
          # First verify basic port connectivity
          if ! nc -zv -w 5 $IP 80; then
            echo "❌ Port 80 not accessible"
            exit 1
          fi
          
          # Then test HTTP endpoint
          for i in {1..15}; do
            RESPONSE=$(curl -sS -m 10 -o /dev/null -w "%{http_code}" "http://$IP")
            if [ "$RESPONSE" = "200" ]; then
              echo "✅ Application responded successfully with HTTP 200!"
              curl -v "http://$IP"
              exit 0
            fi
            echo "Attempt $i: Got HTTP $RESPONSE, waiting..."
            sleep 5
          done
          
          echo "❌ Application did not return 200 OK after 15 attempts"
          exit 1

      - name: Debug on Failure
        if: ${{ failure() }}
        run: |
          IP=${{ needs.provision-infrastructure.outputs.ec2_ip }}
          echo "=== Debug Information ==="
          echo "Port check:"
          nc -zv $IP 80 || echo "Port 80 not accessible"
          echo "Curl verbose output:"
          curl -v "http://$IP" || true
          echo "Traefik logs:"
          ssh -i ~/.ssh/deploy_key ec2-user@$IP \
            "kubectl logs -n kube-system -l app.kubernetes.io/name=traefik --tail=50"
          echo "Application logs:"
          ssh -i ~/.ssh/deploy_key ec2-user@$IP \
            "kubectl logs deployment/re2ect-app --tail=20"
          echo "Cluster events:"
          ssh -i ~/.ssh/deploy_key ec2-user@$IP \
            "kubectl get events -A --sort-by=.metadata.creationTimestamp"